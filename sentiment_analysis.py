# -*- coding: utf-8 -*-
"""Sentiment Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S5vVNNxwmhCcDIzdx5JWXNwdud6QAGZ6
"""

!pip install matplotlib-venn

!apt-get -qq install -y libfluidsynth1

# https://pypi.python.org/pypi/libarchive
!apt-get -qq install -y libarchive-dev && pip install -U libarchive
import libarchive

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

from keras.preprocessing.text import Tokenizer

from keras.models import Sequential
from keras.layers import Embedding, LSTM, GRU, Bidirectional, Dense, Dropout
from keras.callbacks import EarlyStopping

from keras.utils import pad_sequences

data = pd.read_csv("urdu-sentiment-corpus-v1.tsv", sep="\t")

data.head()

data.info()
data.describe()

import warnings
warnings.filterwarnings('ignore')

data.isna().values.any()

data.drop(data[data['Class']=='O'].index,inplace = True)

from sklearn.preprocessing import LabelEncoder
data['Class']=LabelEncoder().fit_transform(data['Class'])
piece = Tokenizer()
piece.fit_on_texts(data['Tweet'])
seq = piece.texts_to_sequences(data['Tweet'])

vocablen = len(piece.word_index) + 1
max_len = max(len(s) for s in seq)
x = pad_sequences(seq, maxlen=max_len, padding='post')
y = data['Class']

xtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size=0.25,random_state=101)

from tensorflow.keras.layers import Dense, Dropout,Flatten,LSTM,Bidirectional,SimpleRNN

dummy1 = Sequential()
em = Embedding(vocablen,100,input_length=max_len,trainable = True)
dummy1.add(em)
dummy1.add(SimpleRNN(100,return_sequences=True ,dropout=0.3))
dummy1.add(Dense(512,activation='tanh'))
dummy1.add(Flatten())
dummy1.add(Dense(1,activation='sigmoid'))
dummy1.compile(loss ="binary_crossentropy", optimizer = 'adam', metrics=["accuracy"])

dummy1.fit(xtrain,ytrain,batch_size=20,epochs = 10)

prediction1 = dummy1.predict(xtest)

import numpy as np
prediction2 = np.argmax(prediction1,axis=1)
ac = accuracy_score(ytest,prediction2)
pres = precision_score(ytest,prediction2,average = 'macro')

f1 = f1_score(ytest,prediction2, average = 'macro')
rec = recall_score(ytest,prediction2, average = 'macro')

dataset = pd.DataFrame({
 'precision':[pres],
 'recall' :[rec],
 'accuracy':[ac],
 'F1_score':[f1]
})

dataset

#BiLSTM
dummy2 = Sequential()
em = Embedding(vocablen,100,input_length=max_len,trainable = True)
dummy2.add(em)
dummy2.add(Bidirectional(LSTM(100,return_sequences=True)))
dummy2.add(Dropout(0.3))
dummy2.add(Dense(2,activation='relu'))
dummy2.add(Flatten())
dummy2.add(Dense(1,activation='sigmoid'))
dummy2.compile(loss ="binary_crossentropy", optimizer = 'sgd', metrics=["accuracy"])

dummy2.summary()

dummy2.fit(xtrain,ytrain,batch_size=30,epochs = 10)

prediction1 = dummy2.predict(xtest)

prediction2 = np.argmax(prediction1,axis=1)
ac = accuracy_score(ytest,prediction2)
pres = precision_score(ytest,prediction2,average = 'macro')
f1 = f1_score(ytest,prediction2, average = 'macro')
rec = recall_score(ytest,prediction2, average = 'macro')

dataset2 = pd.DataFrame({
 'precision':[pres],
 'recall' :[rec],
 'accuracy':[ac],
 'F1_score':[f1]
})

dataset2

#GRU
dummy3 = Sequential()
em = Embedding(vocablen,100,input_length=max_len,trainable = True)
dummy3.add(em)
dummy3.add(GRU(100,return_sequences=True))
dummy3.add(Dense(2,activation='relu'))
dummy3.add(Flatten())
dummy3.add(Dense(1,activation='sigmoid'))
dummy3.compile(loss ="binary_crossentropy", optimizer = 'sgd', metrics=["accuracy"])

dummy3.summary()

dummy3.fit(xtrain,ytrain,batch_size=20,epochs = 10)

prediction1 = dummy3.predict(xtest)

prediction2 = np.argmax(prediction1,axis=1)
ac = accuracy_score(ytest,prediction2)
pres = precision_score(ytest,prediction2,average = 'macro')
f1 = f1_score(ytest,prediction2, average = 'macro')
rec = recall_score(ytest,prediction2, average = 'macro')

dummy3= pd.DataFrame({
 'precision':[pres],
 'recall' :[rec],
 'accuracy':[ac],
 'F1_score':[f1]
})

dummy3

#LSTM
dummy4 = Sequential()
em = Embedding(vocablen,100,input_length=max_len,trainable = True)
dummy4.add(em)
dummy4.add(LSTM(100,return_sequences=True))
dummy4.add(Dropout(0.7))
dummy4.add(Dense(2,activation='relu'))
dummy4.add(Flatten())
dummy4.add(Dense(1,activation='sigmoid'))
dummy4.compile(loss ="binary_crossentropy", optimizer = 'sgd', metrics=["accuracy"])

dummy4.summary()

dummy4.fit(xtrain,ytrain,batch_size=20,epochs = 10)

prediction1= dummy4.predict(xtest)

prediction2 = np.argmax(prediction1,axis=1)
ac = accuracy_score(ytest,prediction2)
pres = precision_score(ytest,prediction2,average = 'macro')
f1 = f1_score(ytest,prediction2, average = 'macro')
rec = recall_score(ytest,prediction2, average = 'macro')

dummy4 = pd.DataFrame({
 'precision':[pres],
 'recall' :[rec],
 'accuracy':[ac],
 'F1_score':[f1]
})

dummy4

